# -*- coding: utf-8 -*-
"""Bagging.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XgDBzSSl5_ix8tYwMxa_ucHxTG2NjClT

# Ensemble Learning: Bagging Tutorial

We will use pima indian diabetes dataset to predict if a person has a diabetes or not based on certain features such as blood pressure, skin thickness, age etc. We will train a standalone model first and then use bagging ensemble technique to check how it can improve the performance of the model
"""

import pandas as pd

df = pd.read_csv("diabetes.csv")
df.head()

df.isnull().sum()

df.describe()

df.Outcome.value_counts()

"""**Train test split**"""

X = df.drop("Outcome",axis="columns")
y = df.Outcome

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled[:3]

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, stratify=y, random_state=10)

X_train.shape

X_test.shape

y_train.value_counts()

201/375

y_test.value_counts()

67/125

"""**Train using stand alone model**"""

from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier

scores = cross_val_score(DecisionTreeClassifier(), X, y, cv=5)
scores

scores.mean()

"""**Train using Bagging**"""

from sklearn.ensemble import BaggingClassifier

bag_model = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=100,
    max_samples=0.8,
    oob_score=True,
    random_state=0
)
bag_model.fit(X_train, y_train)
bag_model.oob_score_

bag_model.score(X_test, y_test)

bag_model = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(),
    n_estimators=100,
    max_samples=0.8,
    oob_score=True,
    random_state=0
)
scores = cross_val_score(bag_model, X, y, cv=5)
scores

scores.mean()

"""**Train using Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

scores = cross_val_score(RandomForestClassifier(n_estimators=50), X, y, cv=5)
scores.mean()

"""# Excercise"""

import pandas as pd

df = pd.read_csv("heart.csv")
df.head()

df.shape

df.describe()

"""**Treat Outliers**"""

df[df.Cholesterol>(df.Cholesterol.mean()+3*df.Cholesterol.std())]

df.shape

df1 = df[df.Cholesterol<=(df.Cholesterol.mean()+3*df.Cholesterol.std())]
df1.shape

df[df.MaxHR>(df.MaxHR.mean()+3*df.MaxHR.std())]

df[df.FastingBS>(df.FastingBS.mean()+3*df.FastingBS.std())]

df[df.Oldpeak>(df.Oldpeak.mean()+3*df.Oldpeak.std())]

df2 = df1[df1.Oldpeak<=(df1.Oldpeak.mean()+3*df1.Oldpeak.std())]
df2.shape

df[df.RestingBP>(df.RestingBP.mean()+3*df.RestingBP.std())]

df3 = df2[df2.RestingBP<=(df2.RestingBP.mean()+3*df2.RestingBP.std())]
df3.shape

df.ChestPainType.unique()

df.RestingECG.unique()

df.ExerciseAngina.unique()

df.ST_Slope.unique()

"""**Handle text columns using label encoding and one hot encoding**"""

df4 = df3.copy()
df4.ExerciseAngina.replace(
    {
        'N': 0,
        'Y': 1
    },
    inplace=True)

df4.ST_Slope.replace(
    {
        'Down': 1,
        'Flat': 2,
        'Up': 3
    },
    inplace=True
)

df4.RestingECG.replace(
    {
        'Normal': 1,
        'ST': 2,
        'LVH': 3
    },
    inplace=True)

df4.head()

df5 = pd.get_dummies(df4, drop_first=True)
df5.head()

X = df5.drop("HeartDisease",axis='columns')
y = df5.HeartDisease

X.head()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=20)

X_train.shape

X_test.shape

"""**Train a model using standalone support vector machine and then using bagging**"""

from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score

scores = cross_val_score(SVC(), X, y, cv=5)
scores.mean()

"""**Use bagging now with svm**"""

from sklearn.ensemble import BaggingClassifier

bag_model = BaggingClassifier(base_estimator=SVC(), n_estimators=100, max_samples=0.8, random_state=0)
scores = cross_val_score(bag_model, X, y, cv=5)
scores.mean()

"""As you can see above, using bagging in case of SVM doesn't make much difference in terms of model accuracy. Bagging is effective when we have high variance and instable model such as decision tree. Let's explore how bagging changes the performance for a decision tree classifier.

**Train a model using decision tree and then using bagging**
"""

from sklearn.tree import DecisionTreeClassifier

scores = cross_val_score(DecisionTreeClassifier(random_state=0), X, y, cv=5)
scores.mean()

"""**Use bagging now with decision tree**"""

bag_model = BaggingClassifier(
    base_estimator=DecisionTreeClassifier(random_state=0),
    n_estimators=100,
    max_samples=0.9,
    oob_score=True,
    random_state=0
)

scores = cross_val_score(bag_model, X, y, cv=5)
scores.mean()

"""You can see that with bagging the score improved from 71.93% to 80.37%**bold text**

**Train a model using Random Forest which itself uses bagging underneath**
"""

from sklearn.ensemble import RandomForestClassifier

scores = cross_val_score(RandomForestClassifier(), X, y, cv=5)
scores.mean()

"""**Random forest gave even a better performance with 81.7% as score. Underneath it used bagging where it sampled not only data rows but also the columns (or features)**"""