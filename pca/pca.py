# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M_nHToy1RAtQf6cu-fu1IDO8lewZsorF

# Principal Component Analysis
"""

from sklearn.datasets import load_digits
import pandas as pd

dataset = load_digits()
dataset.keys()

dataset.data.shape

dataset.data[0]

dataset.data[0].reshape(8,8)

# Commented out IPython magic to ensure Python compatibility.
from matplotlib import pyplot as plt
# %matplotlib inline
plt.gray()
plt.matshow(dataset.data[0].reshape(8,8))

plt.matshow(dataset.data[9].reshape(8,8))

dataset.target[:5]

df = pd.DataFrame(dataset.data, columns=dataset.feature_names)
df.head()

dataset.target

df.describe()

X = df
y = dataset.target

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=30)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X_train, y_train)
model.score(X_test, y_test)

"""**Use PCA to reduce dimensions**"""

X

"""**Use components such that 95% of variance is retained**"""

from sklearn.decomposition import PCA

pca = PCA(0.95)
X_pca = pca.fit_transform(X)
X_pca.shape

pca.explained_variance_ratio_

pca.n_components_

"""**PCA created 29 components out of 64 original columns**"""

X_pca

X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=1000)
model.fit(X_train_pca, y_train)
model.score(X_test_pca, y_test)

"""**Let's now select only two components**"""

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)
X_pca.shape

X_pca

pca.explained_variance_ratio_

"""**You can see that both combined retains 0.14+0.13=0.27 or 27% of important feature information**"""

X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)

model = LogisticRegression(max_iter=1000)
model.fit(X_train_pca, y_train)
model.score(X_test_pca, y_test)

"""We get less accuancy (~60%) as using only 2 components did not retain much of the feature information. However in real life you will find many cases where using 2 or few PCA components can still give you a pretty good accuracy

# Excercise

**PCA Machine Learning**
"""

import pandas as pd

# https://www.kaggle.com/fedesoriano/heart-failure-prediction
df = pd.read_csv("heart.csv")
df.head()

df.shape

df.describe()

"""**Treat Outliers**"""

df[df.Cholesterol>(df.Cholesterol.mean()+3*df.Cholesterol.std())]

df.shape

df1 = df[df.Cholesterol<=(df.Cholesterol.mean()+3*df.Cholesterol.std())]
df1.shape

df[df.MaxHR>(df.MaxHR.mean()+3*df.MaxHR.std())]

df[df.FastingBS>(df.FastingBS.mean()+3*df.FastingBS.std())]

df[df.Oldpeak>(df.Oldpeak.mean()+3*df.Oldpeak.std())]

df2 = df1[df1.Oldpeak<=(df1.Oldpeak.mean()+3*df1.Oldpeak.std())]
df2.shape

df[df.RestingBP>(df.RestingBP.mean()+3*df.RestingBP.std())]

df3 = df2[df2.RestingBP<=(df2.RestingBP.mean()+3*df2.RestingBP.std())]
df3.shape

df.ChestPainType.unique()

df.RestingECG.unique()

df.ExerciseAngina.unique()

df.ST_Slope.unique()

df4 = df3.copy()
df4.ExerciseAngina.replace(
    {
        'N': 0,
        'Y': 1
    },
    inplace=True)

df4.ST_Slope.replace(
    {
        'Down': 1,
        'Flat': 2,
        'Up': 3
    },
    inplace=True
)

df4.RestingECG.replace(
    {
        'Normal': 1,
        'ST': 2,
        'LVH': 3
    },
    inplace=True)

df4.head()

df5 = pd.get_dummies(df4, drop_first=True)
df5.head()

X = df5.drop("HeartDisease",axis='columns')
y = df5.HeartDisease

X.head()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=30)

X_train.shape

X_test.shape

from sklearn.ensemble import RandomForestClassifier

model_rf = RandomForestClassifier()
model_rf.fit(X_train, y_train)
model_rf.score(X_test, y_test)

"""**Use PCA to reduce dimensions**"""

X

from sklearn.decomposition import PCA

pca = PCA(0.95)
X_pca = pca.fit_transform(X)
X_pca

X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=30)

from sklearn.ensemble import RandomForestClassifier

model_rf = RandomForestClassifier()
model_rf.fit(X_train_pca, y_train)
model_rf.score(X_test_pca, y_test)